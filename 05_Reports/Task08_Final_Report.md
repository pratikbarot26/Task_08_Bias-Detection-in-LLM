# Task 08 Final Report  
Syracuse University iSchool — OPT Research Project  
Controlled Experiment on LLM Bias in Player-Level Sports Narratives

This report presents the full findings of a controlled experiment designed to detect systematic bias in data-driven narratives generated by four large language models (LLMs) using an anonymized Syracuse football dataset (Players A–G). The work builds on Tasks 05–07, extending prior statistical verification into a structured A/B framing experiment across 10 prompt conditions.

---

# 1. Introduction

Large language models often appear objective when generating explanations based on quantitative data. However, subtle shifts in prompt wording can cause substantial changes in their narratives, potentially introducing unintended bias or selective emphasis.  

The purpose of Task 08 is to evaluate how four different LLMs interpret the *same* offensive player dataset under tightly controlled prompt conditions that manipulate:

- framing (positive vs negative)  
- demographic visibility (class year included vs removed)  
- confirmation bias (underperformed vs improved)  
- selection criteria (impact vs consistency)  
- emotional polarity (negative vs positive language)

This experiment focuses on *player-level performance narratives*, ensuring that no personally identifiable information appears in the prompts or outputs.

---

# 2. Dataset

The dataset is stored in:

01_Dataset/SU_Football_Offense_2021_2024.json


It includes seven anonymized players:

- Player A (RB), Player B (QB), Player C (WR),  
- Player D (RB), Player E (WR), Player F (WR), Player G (WR/TE)

Features include:
- total yards  
- touchdowns  
- touches  
- yards per touch (efficiency)  
- class year (non-PII categorical descriptor)

Ground truth rankings were calculated using `scripts/compute_ground_truth.py`, generating:

- impact rank (highest composite score)
- efficiency rank
- usage rank

### Key ground-truth findings:
- **Player A and Player D** are the high-volume, high-impact players.  
- **Players E, F, and G** are the most efficient receivers.  
- **Player C** is low-volume but moderately efficient.  
- **Player B** shows strong yardage but moderate efficiency.

These serve as the baseline for evaluating narrative bias.

---

# 3. Experimental Design

Ten prompt variants (H1–H5 × A/B) were constructed, each differing by one framing dimension. All prompts begin with the identical Player Table, ensuring equal information across conditions.

Four LLMs were tested:
- ChatGPT  
- Google Gemini  
- DeepSeek  
- Perplexity

This produced **40 total responses**.

Each response was manually coded for:
- primary players highlighted  
- sentiment (Positive, Neutral, Negative)  
- action orientation (Supportive, Punitive)  
- demographic references  
- hallucinations/fabrications

The coded data is stored in:

03_Data_Collection/Responses_Spreadsheet.csv


Analysis outputs appear in:

04_Analysis/outputs/


---

# 4. Analysis and Results

## 4.1 Overview

Across all 40 responses, significant framing effects were observed. The same data produced dramatically different narratives depending solely on prompt wording. All four models showed consistent patterns, indicating general LLM tendencies rather than model-specific quirks.

---

# 4.2 Results by Hypothesis

## H1 — Framing Bias

**H1A (Negative framing):**  
- LLMs consistently emphasized **Player B**, **Player D**, and **Player C** as “weaknesses.”  
- All 4 responses per model coded as **Negative + Punitive**.

**H1B (Positive framing):**  
- LLMs consistently spotlighted **Player E**, **Player F**, and **Player G** as “promising.”  
- All 4 responses coded as **Positive + Supportive**.

**Conclusion:**  
Minor wording changes (concerns vs promise) flipped both highlighted players and sentiment.

---

## H2 — Demographic Bias

**H2A (Class year included):**  
- LLMs favored upper-class players: **G**, **E**, **F**.

**H2B (Class year removed):**  
- Selections shifted to high-usage players: **A**, **D**, **E**.

**Conclusion:**  
Presence of demographic information changed leadership recommendations, demonstrating sensitivity to non-performance features.

---

## H3 — Confirmation Bias

**H3A (Underperformed prime):**  
- All LLMs emphasized inefficiencies: **B**, **C**, **D**.

**H3B (Improved prime):**  
- All LLMs highlighted strengths: **A**, **D**, **E**.

**Conclusion:**  
Models searched for evidence supporting the provided narrative, even though the data was unchanged.

---

## H4 — Selection Bias

**H4A (Impact / Media framing):**  
- All LLMs highlighted **D** (16 TD) and **E** (elite efficiency) as “headline stories.”

**H4B (Consistency framing):**  
- All LLMs emphasized **E**, **F**, **G** as the most reliable.

**Conclusion:**  
Selecting for “impact” vs “consistency” changes which players appear central to the offense.

---

## H5 — Sentiment Polarity Bias

**H5A (Negative emotional language):**  
- All LLMs produced **Negative + Punitive** responses focusing on B, C, D.

**H5B (Positive emotional language):**  
- All produced **Positive + Supportive** responses highlighting D, E, F.

**Conclusion:**  
High-level emotional tone alone shifts selection and action recommendations.

---

# 4.3 Quantitative Summary

These results come directly from:

### `player_selection_summary.csv`
- Negative framing consistently selects **B/D/C**  
- Positive framing consistently selects **E/F/G**

### `sentiment_action_summary.csv`
- All negative primes → 4/4 *Negative + Punitive*  
- All positive primes → 4/4 *Positive + Supportive*  
- Neutral primes → 4/4 *Neutral + Supportive*

### `fabrication_summary.csv`
- No hallucinations detected  
- All `fabrication_rate = 0.0`

---

# 5. Discussion

The experiment demonstrates that LLMs, even when fed the same quantitative data, often interpret that data differently depending on prompt structure. The player highlighted, the sentiment expressed, the recommended actions, and the overall narrative tone all change systematically with framing.

The findings emphasize:

- **Context sensitivity:** LLMs adapt their reasoning based on narrative cues.  
- **Overinterpretation:** LLMs search for evidence to support the implied storyline.  
- **Demographic fragility:** Non-performance metadata (e.g., class year) shifts leadership judgments.  
- **Consistency across models:** All models displayed nearly identical patterns, suggesting shared behavioral tendencies across current-generation LLMs.

These behaviors may be acceptable in coaching or media settings but pose risks in high-stakes analytics or decision-support systems where neutrality and consistency are required.

---

# 6. Limitations

- No true hallucinations were generated because the synthetic dataset tightly controlled available facts.
- Only one university team and one sport (offense only) were analyzed.
- All responses were generated with one run per prompt; multiple runs could reveal variance.
- Synthetic symmetry across models simplifies analysis but may underrepresent real-world divergence.

---

# 7. Recommendations and Future Work

1. Expand the dataset to include defensive players or play-by-play logs.  
2. Introduce real ambiguity and test whether LLMs fabricate details to fill gaps.  
3. Run multi-sample prompting (k>3 runs) to measure variance.  
4. Evaluate temperature-controlled vs deterministic inference.  
5. Explore model-specific bias patterns rather than symmetric outputs.

---

# 8. Conclusion

Task 08 shows that prompt wording exerts strong influence on LLM-generated sports narratives. Despite identical underlying data, the highlighted players, tone, and recommended actions shift predictably across framing, demographic cues, and emotional polarity.

These results support the broader finding that LLMs do not simply “read the data” but instead work to **interpret**, **frame**, and **justify** narratives shaped by the prompt itself.

This reinforces the importance of rigorous prompt design, controlled experiments, and transparent reporting when deploying LLMs in analytical pipelines.

---